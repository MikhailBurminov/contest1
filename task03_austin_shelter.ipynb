{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Кратко об этом исследовании\n",
    "Данные из Остинского центра животных, то есть приюта, -- с 1 октября 2013 по **март 2016**. \n",
    "\n",
    "### Цель\n",
    "\n",
    "Требуется предсказать судьбу каждого животного по данным о нём сведениям. По сути, обычная задача категоризации. Классы: Adoption, Died, Euthanasia, Return to owner, Transfer. \n",
    "\n",
    "Все классы считаем одинаково важными все зависимости от представленности в выборке. Качество предсказаний оценивается поэтому с помощью macro-averaged F1 score.\n",
    "\n",
    "---\n",
    "\n",
    "**Задание**\n",
    "\n",
    "Пользоваться в точности предложенной в этом шаблоне схемой необязательно, но в этом ноутбуке должен быть \n",
    "- внятный и чистый, \n",
    "- прокомментированный, \n",
    "- воспроизводимый (зафиксируйте все random seeds, которые возможно),\n",
    "- мотивированный\n",
    "**код**, который **генерирует ваше лучшее решение**.\n",
    "\n",
    "А также пока **неформальный отчёт** о проделанной работе.\n",
    "\n",
    "\n",
    "### Методы\n",
    "\n",
    "`TODO: Напишите, как пробовали предобрабатывать признаки`\n",
    "\n",
    "Постарался сразу выделить все признаки, что можно было, и не возвращаться. Разделил животных на тех, у кого есть имя и у кого нет. Неизвестный срок заполнил средним значением. Объединил мало встречающиеся редкие породы и цвета. Пробовал выделять щенков и котят в отдельный признак - результата не дало, хотя было бы логично, возможно, что-то сделал не так. Стерилизованных животных тоже объединил.\n",
    "\n",
    "\n",
    "`TODO:Напишите, какие модели и с какими параметрами вы пробовали`\n",
    "\n",
    "Попробовал две модели, предложенную DecisionTreeClassifier и XGBClassifier от XGBoost.\n",
    "\n",
    "Тактика выбора параметров у меня одна: сначала берем широкий диапазон параметров, смотрим максимум, а дальше наблюдаем, какие изменения ведут к улучшению скора, пытаемся их оптимизировать. Иногда направлений движения несколько, смотрим все. В DecisionTreeClassifier заметил, что любые параметры max_features вели к ухудшению скора. Этот результат сохраняется в submission1.csv.\n",
    "\n",
    "`Cross-validation f1 macro: 0.422 (+/- 0.021) for 'criterion': 'entropy', 'max_depth': 18, 'min_samples_leaf': 1, 'min_samples_split': 2, 'splitter': 'best'`\r\n",
    "\n",
    "В XGBClassifier тактика такая же. Здесь я усмотрел два направления движения, которые давали похожий результат.\n",
    "Первый это увеличивать n_estimators, увеличивать max_depth и добавлять регуляризацию в виде gamma, reg_alpha. Такой путь занимает больше времени на обучение. Второй это иметь маленький n_estimators и max_depth пониже c незначительной регуляризацией. Модель быстро учится и даже дает чуть лучший результат в итоге. Оба варианта я оставил в коде, закоментив первый. Этот вериант сохраняется в submission.csv.\n",
    "\n",
    "Первый вариант:  \n",
    "`Cross-validation f1 macro: 0.448 (+/- 0.024) for 'gamma': 0.04, 'learning_rate': 0.3, 'max_depth': 13, 'n_estimators': 300, 'reg_alpha': 0.05`\n",
    "\n",
    "Второй вариант:  \n",
    "`Cross-validation f1 macro: 0.455 (+/- 0.020) for 'gamma': 0.005, 'learning_rate': 0.3, 'max_depth': 11, 'n_estimators': 50, 'reg_alpha': 0.01`\n",
    "\n",
    "Еще была идея попробовтать RandomForest, LightBM (но кажется это примерно то же самое, что и XGBClassifier), а потом ввести голосование по всем моделям, но руки не дошли.\n",
    "\n",
    "\n",
    "### Результаты\n",
    "\n",
    "`TODO: Поделитесь наблюдениями, историями успеха и зря потраченными усилиями; что интересного можете сказать о наборе данных? какие выводы?`\n",
    "\n",
    "Больше времени я потратил на обучение XGBClassifier, потому что подумал, что она более сложная и должна выдавать лучший результат, так и получилось. В какой-то момент возникло ощущение, что у меня появился потолок в скоре, который я обьяснял себе переобучением, но введение регуляризации этому не особо помогло. Зря потраченными усилиями оказалось увеличение параметров и долгое обучение XGBClassifier. Сравнительное низкие простые параметры все равно показывали лучший результат, видимо, переобучение. Вывод, что иногда лучше понизить сложность модели, чем вводить регуляризацию. По самим данным возникло ощущение, что их довольно мало для каких-то сложных моделей.\n",
    "\n",
    "---\n",
    "---\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Конфиги и константы\n",
    "(пожалуйста, без волшебных чисел в коде)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "OUTCOME2LABEL = {\"Adoption\" : 0, \n",
    "                 \"Transfer\": 1, \n",
    "                 \"Return_to_owner\": 2, \n",
    "                 \"Euthanasia\": 3, \n",
    "                 \"Died\": 4\n",
    "                }\n",
    "LABEL2OUTCOME = {v: k for k,v in OUTCOME2LABEL.items()}\n",
    "FOLD_K = 4\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Библиотеки\n",
    "(все импорты желательно должны быть здесь)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'1.4.2'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import sklearn\n",
    "sklearn.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#pip install xgboost \n",
    "# Нужно установить XGBoost, я его использовал"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import xgboost\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "\n",
    "from sklearn import tree\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.model_selection import GridSearchCV, StratifiedKFold, cross_val_score\n",
    "from sklearn.metrics import f1_score, classification_report, confusion_matrix\n",
    "from xgboost import XGBClassifier, plot_importance, plot_tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Name              7691\n",
      "SexuponOutcome       1\n",
      "AnimalType           0\n",
      "AgeuponOutcome      18\n",
      "Breed                0\n",
      "Color                0\n",
      "DateTime             0\n",
      "Outcome           8019\n",
      "ID                   0\n",
      "IsTrain              0\n",
      "dtype: int64\n",
      "      Name SexuponOutcome AnimalType AgeuponOutcome                    Breed  \\\n",
      "0    Socks  Neutered Male        Cat       2 months   Domestic Shorthair Mix   \n",
      "1     Vera  Intact Female        Cat        1 month   Domestic Shorthair Mix   \n",
      "2  Biscuit  Neutered Male        Dog       3 months  Chihuahua Shorthair Mix   \n",
      "3   Kitten  Spayed Female        Cat        2 years   Domestic Shorthair Mix   \n",
      "4      NaN  Neutered Male        Cat       2 months   Domestic Shorthair Mix   \n",
      "\n",
      "          Color             DateTime  Outcome  ID  IsTrain  \n",
      "0   Black/White  2014-06-11 14:36:00      0.0   0        1  \n",
      "1  Tortie/White  2014-07-18 08:10:00      3.0   1        1  \n",
      "2        Yellow  2016-01-02 17:28:00      2.0   2        1  \n",
      "3        Calico  2014-02-19 17:27:00      0.0   3        1  \n",
      "4  Orange Tabby  2014-07-21 17:34:00      0.0   4        1  \n"
     ]
    }
   ],
   "source": [
    "train_data = pd.read_csv(\"train.csv\", encoding=\"utf-8\")\n",
    "test_data = pd.read_csv(\"test.csv\", encoding=\"utf-8\")\n",
    "\n",
    "# Объединим трейн и тест, чтобы вместе их обработать\n",
    "train_data['IsTrain'] = 1\n",
    "test_data['IsTrain'] = 0\n",
    "\n",
    "test_data['Outcome'] = np.nan\n",
    "\n",
    "combined_data = pd.concat([train_data, test_data], ignore_index=True)\n",
    "\n",
    "print(combined_data.isnull().sum())\n",
    "print(combined_data.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Подготовка признаков\n",
    "\n",
    "#### Даты"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pandas_dates2number(date_series: pd.Series):\n",
    "    return pd.to_datetime(date_series).values.astype(np.int64) // 10 ** 9\n",
    "\n",
    "combined_data['DateTime'] = pandas_dates2number(combined_data['DateTime'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Имена\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  SexuponOutcome AnimalType AgeuponOutcome                    Breed  \\\n",
      "0  Neutered Male        Cat       2 months   Domestic Shorthair Mix   \n",
      "1  Intact Female        Cat        1 month   Domestic Shorthair Mix   \n",
      "2  Neutered Male        Dog       3 months  Chihuahua Shorthair Mix   \n",
      "3  Spayed Female        Cat        2 years   Domestic Shorthair Mix   \n",
      "4  Neutered Male        Cat       2 months   Domestic Shorthair Mix   \n",
      "\n",
      "          Color    DateTime  Outcome  ID  IsTrain  HasName  \n",
      "0   Black/White  1402497360      0.0   0        1        1  \n",
      "1  Tortie/White  1405671000      3.0   1        1        1  \n",
      "2        Yellow  1451755680      2.0   2        1        1  \n",
      "3        Calico  1392830820      0.0   3        1        1  \n",
      "4  Orange Tabby  1405964040      0.0   4        1        0  \n"
     ]
    }
   ],
   "source": [
    "combined_data['HasName'] = (~combined_data['Name'].isnull()).astype(int) # Есть имя или нет\n",
    "combined_data.drop('Name', axis=1, inplace=True)\n",
    "print(combined_data.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Возраст\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  SexuponOutcome AnimalType                    Breed         Color  \\\n",
      "0  Neutered Male        Cat   Domestic Shorthair Mix   Black/White   \n",
      "1  Intact Female        Cat   Domestic Shorthair Mix  Tortie/White   \n",
      "2  Neutered Male        Dog  Chihuahua Shorthair Mix        Yellow   \n",
      "3  Spayed Female        Cat   Domestic Shorthair Mix        Calico   \n",
      "4  Neutered Male        Cat   Domestic Shorthair Mix  Orange Tabby   \n",
      "\n",
      "     DateTime  Outcome  ID  IsTrain  HasName  AgeInDays  \n",
      "0  1402497360      0.0   0        1        1       60.0  \n",
      "1  1405671000      3.0   1        1        1       30.0  \n",
      "2  1451755680      2.0   2        1        1       90.0  \n",
      "3  1392830820      0.0   3        1        1      730.0  \n",
      "4  1405964040      0.0   4        1        0       60.0  \n"
     ]
    }
   ],
   "source": [
    "def convert_age_to_days(age_str):\n",
    "    if age_str == 'Unknown' or pd.isnull(age_str):\n",
    "        return np.nan\n",
    "    age_num, age_unit = age_str.split()\n",
    "    age_num = float(age_num)\n",
    "    if 'year' in age_unit:\n",
    "        return age_num * 365\n",
    "    elif 'month' in age_unit:\n",
    "        return age_num * 30\n",
    "    elif 'week' in age_unit:\n",
    "        return age_num * 7\n",
    "    elif 'day' in age_unit:\n",
    "        return age_num\n",
    "    else:\n",
    "        return np.nan\n",
    "# Сделаем возраст числом\n",
    "combined_data['AgeInDays'] = combined_data['AgeuponOutcome'].apply(convert_age_to_days)\n",
    "# Заполним средним отсутсвующие данные о сроке\n",
    "median_age = combined_data[combined_data['IsTrain'] == 1]['AgeInDays'].median() \n",
    "combined_data['AgeInDays'].fillna(median_age, inplace=True)\n",
    "combined_data.drop('AgeuponOutcome', axis=1, inplace=True)\n",
    "\n",
    "\n",
    "print(combined_data.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Цвета и породы\n",
    "\n",
    "Повторяющихся категорий много, так что можно закодировать их и так, однако **некоторые из них можно и растащить на части**\n",
    "\n",
    "Необязательно, но можете попробовать, вдруг поможет :)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PrimaryBreed\n",
      "Domestic Shorthair        6242\n",
      "Chihuahua Shorthair       1495\n",
      "Pit Bull                  1471\n",
      "Labrador Retriever        1375\n",
      "Domestic Medium Hair       622\n",
      "                          ... \n",
      "Papillon                    15\n",
      "Vizsla                      15\n",
      "Harrier                     15\n",
      "Shiba Inu                   15\n",
      "Parson Russell Terrier      15\n",
      "Name: count, Length: 89, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# Выделим смешанные породы, разделим их. Редкие бороды объединим\n",
    "combined_data['IsMix'] = combined_data['Breed'].str.contains('Mix|/', regex=True).astype(int)\n",
    "combined_data['PrimaryBreed'] = combined_data['Breed'].str.split('/').str[0]\n",
    "combined_data['PrimaryBreed'] = combined_data['PrimaryBreed'].str.replace(' Mix', '', regex=False)\n",
    "\n",
    "breed_counts = combined_data[combined_data['IsTrain'] == 1]['PrimaryBreed'].value_counts()\n",
    "\n",
    "rare_breeds = breed_counts[breed_counts < 15].index\n",
    "combined_data['PrimaryBreed'] = combined_data['PrimaryBreed'].replace(rare_breeds, 'Other')\n",
    "combined_data.drop('Breed', axis=1, inplace=True)\n",
    "\n",
    "print(combined_data[combined_data['IsTrain'] == 1]['PrimaryBreed'].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PrimaryColor\n",
      "Black              4491\n",
      "White              2350\n",
      "Brown Tabby        1813\n",
      "Brown              1358\n",
      "Tan                1173\n",
      "Orange Tabby        911\n",
      "Blue                820\n",
      "Tricolor            572\n",
      "Red                 557\n",
      "Blue Tabby          476\n",
      "Brown Brindle       473\n",
      "Tortie              409\n",
      "Calico              383\n",
      "Chocolate           321\n",
      "Torbie              276\n",
      "Sable               215\n",
      "Buff                199\n",
      "Cream Tabby         179\n",
      "Cream               172\n",
      "Yellow              156\n",
      "Gray                155\n",
      "Fawn                145\n",
      "Lynx Point          124\n",
      "Seal Point          119\n",
      "Blue Merle          115\n",
      "Black Brindle        66\n",
      "Flame Point          64\n",
      "Gold                 56\n",
      "Black Smoke          50\n",
      "Brown Merle          49\n",
      "Black Tabby          44\n",
      "Gray Tabby           38\n",
      "Red Merle            36\n",
      "Silver               35\n",
      "Silver Tabby         32\n",
      "Blue Tick            29\n",
      "Orange               27\n",
      "Lilac Point          24\n",
      "Yellow Brindle       23\n",
      "Blue Point           23\n",
      "Red Tick             23\n",
      "Tortie Point         22\n",
      "Other                22\n",
      "Apricot              20\n",
      "Calico Point         19\n",
      "Chocolate Point      18\n",
      "Blue Cream           15\n",
      "Liver                13\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# Обработаем цвета, объединим редкие\n",
    "combined_data['NumColors'] = combined_data['Color'].str.count('/') + 1\n",
    "combined_data['PrimaryColor'] = combined_data['Color'].str.split('/').str[0]\n",
    "color_counts = combined_data[combined_data['IsTrain'] == 1]['PrimaryColor'].value_counts()\n",
    "rare_colors = color_counts[color_counts < 10].index\n",
    "combined_data['PrimaryColor'] = combined_data['PrimaryColor'].replace(rare_colors, 'Other')\n",
    "\n",
    "print(combined_data[combined_data['IsTrain'] == 1]['PrimaryColor'].value_counts())\n",
    "\n",
    "combined_data.drop('Color', axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Пол\n",
    "\n",
    "Пола, как мы видим, четыре: стерилизованные и нестерилизованные самки и самцы. Также пол может быть неизвестен.\n",
    "\n",
    "Может, факт стерилизации стоит сделать отдельным признаком? Но это неточно."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SexuponOutcome       1\n",
      "AnimalType           0\n",
      "DateTime             0\n",
      "Outcome           8019\n",
      "ID                   0\n",
      "IsTrain              0\n",
      "HasName              0\n",
      "AgeInDays            0\n",
      "IsMix                0\n",
      "PrimaryBreed         0\n",
      "NumColors            0\n",
      "PrimaryColor         0\n",
      "dtype: int64\n",
      "      AnimalType    DateTime  Outcome    ID  IsTrain  HasName  AgeInDays  \\\n",
      "0            Cat  1402497360      0.0     0        1        1       60.0   \n",
      "1            Cat  1405671000      3.0     1        1        1       30.0   \n",
      "2            Dog  1451755680      2.0     2        1        1       90.0   \n",
      "3            Cat  1392830820      0.0     3        1        1      730.0   \n",
      "4            Cat  1405964040      0.0     4        1        0       60.0   \n",
      "...          ...         ...      ...   ...      ...      ...        ...   \n",
      "26724        Dog  1428773400      NaN  8014        0        1      240.0   \n",
      "26725        Dog  1444659360      NaN  8015        0        1     3285.0   \n",
      "26726        Dog  1418833500      NaN  8016        0        1     1095.0   \n",
      "26727        Cat  1410374880      NaN  8017        0        0       21.0   \n",
      "26728        Dog  1446973380      NaN  8018        0        0      730.0   \n",
      "\n",
      "       IsMix         PrimaryBreed  NumColors    PrimaryColor              Sex  \n",
      "0          1   Domestic Shorthair          2           Black  Neutered/Spayed  \n",
      "1          1   Domestic Shorthair          2          Tortie           Intact  \n",
      "2          1  Chihuahua Shorthair          1          Yellow  Neutered/Spayed  \n",
      "3          1   Domestic Shorthair          1          Calico  Neutered/Spayed  \n",
      "4          1   Domestic Shorthair          1    Orange Tabby  Neutered/Spayed  \n",
      "...      ...                  ...        ...             ...              ...  \n",
      "26724      1             Pit Bull          2   Brown Brindle  Neutered/Spayed  \n",
      "26725      1  Chihuahua Shorthair          1             Tan           Intact  \n",
      "26726      1             Pit Bull          2  Yellow Brindle  Neutered/Spayed  \n",
      "26727      1   Domestic Shorthair          1     Brown Tabby           Intact  \n",
      "26728      0          Rat Terrier          1        Tricolor           Intact  \n",
      "\n",
      "[26729 rows x 12 columns]\n"
     ]
    }
   ],
   "source": [
    "# Заполним отстуствующие данные пола\n",
    "print(combined_data.isnull().sum())\n",
    "combined_data['SexuponOutcome'].fillna('Unknown', inplace=True)\n",
    "\n",
    "# Объединим стерилизованных\n",
    "def simplify_sex(sex):\n",
    "    if sex in ['Neutered Male', 'Spayed Female']:\n",
    "        return 'Neutered/Spayed'\n",
    "    elif sex in ['Intact Male', 'Intact Female']:\n",
    "        return 'Intact'\n",
    "    else:\n",
    "        return 'Unknown'\n",
    "\n",
    "combined_data['Sex'] = combined_data['SexuponOutcome'].apply(simplify_sex)\n",
    "combined_data.drop('SexuponOutcome', axis=1, inplace=True)\n",
    "\n",
    "print(combined_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "categorical_features = ['AnimalType', 'Sex', 'PrimaryBreed', 'PrimaryColor']\n",
    "combined_data = pd.get_dummies(combined_data, columns=categorical_features) # Воспользуемся простым методом\n",
    "\n",
    "train_data_processed = combined_data[combined_data['IsTrain'] == 1].drop('IsTrain', axis=1)\n",
    "test_data_processed = combined_data[combined_data['IsTrain'] == 0].drop(['IsTrain', 'Outcome'], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = train_data_processed.drop(['Outcome', 'ID'], axis=1)\n",
    "y_train = train_data_processed['Outcome'].astype(int)\n",
    "X_test = test_data_processed.drop('ID', axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### DecisionTreeClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 4 folds for each of 72 candidates, totalling 288 fits\n",
      "Best params on dev set:\n",
      "{'criterion': 'entropy', 'max_depth': 18, 'min_samples_leaf': 1, 'min_samples_split': 2, 'splitter': 'best'}\n",
      "Scores on development set:\n",
      "0.411 (+/-0.020) for {'criterion': 'gini', 'max_depth': 20, 'min_samples_leaf': 1, 'min_samples_split': 2, 'splitter': 'best'}\n",
      "0.411 (+/-0.018) for {'criterion': 'gini', 'max_depth': 20, 'min_samples_leaf': 1, 'min_samples_split': 2, 'splitter': 'random'}\n",
      "0.404 (+/-0.017) for {'criterion': 'gini', 'max_depth': 20, 'min_samples_leaf': 1, 'min_samples_split': 5, 'splitter': 'best'}\n",
      "0.402 (+/-0.020) for {'criterion': 'gini', 'max_depth': 20, 'min_samples_leaf': 1, 'min_samples_split': 5, 'splitter': 'random'}\n",
      "0.396 (+/-0.012) for {'criterion': 'gini', 'max_depth': 20, 'min_samples_leaf': 3, 'min_samples_split': 2, 'splitter': 'best'}\n",
      "0.391 (+/-0.009) for {'criterion': 'gini', 'max_depth': 20, 'min_samples_leaf': 3, 'min_samples_split': 2, 'splitter': 'random'}\n",
      "0.396 (+/-0.012) for {'criterion': 'gini', 'max_depth': 20, 'min_samples_leaf': 3, 'min_samples_split': 5, 'splitter': 'best'}\n",
      "0.391 (+/-0.009) for {'criterion': 'gini', 'max_depth': 20, 'min_samples_leaf': 3, 'min_samples_split': 5, 'splitter': 'random'}\n",
      "0.392 (+/-0.014) for {'criterion': 'gini', 'max_depth': 20, 'min_samples_leaf': 5, 'min_samples_split': 2, 'splitter': 'best'}\n",
      "0.381 (+/-0.014) for {'criterion': 'gini', 'max_depth': 20, 'min_samples_leaf': 5, 'min_samples_split': 2, 'splitter': 'random'}\n",
      "0.392 (+/-0.014) for {'criterion': 'gini', 'max_depth': 20, 'min_samples_leaf': 5, 'min_samples_split': 5, 'splitter': 'best'}\n",
      "0.381 (+/-0.014) for {'criterion': 'gini', 'max_depth': 20, 'min_samples_leaf': 5, 'min_samples_split': 5, 'splitter': 'random'}\n",
      "0.414 (+/-0.026) for {'criterion': 'gini', 'max_depth': 18, 'min_samples_leaf': 1, 'min_samples_split': 2, 'splitter': 'best'}\n",
      "0.405 (+/-0.016) for {'criterion': 'gini', 'max_depth': 18, 'min_samples_leaf': 1, 'min_samples_split': 2, 'splitter': 'random'}\n",
      "0.409 (+/-0.024) for {'criterion': 'gini', 'max_depth': 18, 'min_samples_leaf': 1, 'min_samples_split': 5, 'splitter': 'best'}\n",
      "0.401 (+/-0.023) for {'criterion': 'gini', 'max_depth': 18, 'min_samples_leaf': 1, 'min_samples_split': 5, 'splitter': 'random'}\n",
      "0.396 (+/-0.018) for {'criterion': 'gini', 'max_depth': 18, 'min_samples_leaf': 3, 'min_samples_split': 2, 'splitter': 'best'}\n",
      "0.389 (+/-0.038) for {'criterion': 'gini', 'max_depth': 18, 'min_samples_leaf': 3, 'min_samples_split': 2, 'splitter': 'random'}\n",
      "0.396 (+/-0.018) for {'criterion': 'gini', 'max_depth': 18, 'min_samples_leaf': 3, 'min_samples_split': 5, 'splitter': 'best'}\n",
      "0.389 (+/-0.038) for {'criterion': 'gini', 'max_depth': 18, 'min_samples_leaf': 3, 'min_samples_split': 5, 'splitter': 'random'}\n",
      "0.393 (+/-0.015) for {'criterion': 'gini', 'max_depth': 18, 'min_samples_leaf': 5, 'min_samples_split': 2, 'splitter': 'best'}\n",
      "0.386 (+/-0.016) for {'criterion': 'gini', 'max_depth': 18, 'min_samples_leaf': 5, 'min_samples_split': 2, 'splitter': 'random'}\n",
      "0.393 (+/-0.015) for {'criterion': 'gini', 'max_depth': 18, 'min_samples_leaf': 5, 'min_samples_split': 5, 'splitter': 'best'}\n",
      "0.386 (+/-0.016) for {'criterion': 'gini', 'max_depth': 18, 'min_samples_leaf': 5, 'min_samples_split': 5, 'splitter': 'random'}\n",
      "0.420 (+/-0.010) for {'criterion': 'gini', 'max_depth': 15, 'min_samples_leaf': 1, 'min_samples_split': 2, 'splitter': 'best'}\n",
      "0.406 (+/-0.033) for {'criterion': 'gini', 'max_depth': 15, 'min_samples_leaf': 1, 'min_samples_split': 2, 'splitter': 'random'}\n",
      "0.414 (+/-0.012) for {'criterion': 'gini', 'max_depth': 15, 'min_samples_leaf': 1, 'min_samples_split': 5, 'splitter': 'best'}\n",
      "0.404 (+/-0.018) for {'criterion': 'gini', 'max_depth': 15, 'min_samples_leaf': 1, 'min_samples_split': 5, 'splitter': 'random'}\n",
      "0.402 (+/-0.012) for {'criterion': 'gini', 'max_depth': 15, 'min_samples_leaf': 3, 'min_samples_split': 2, 'splitter': 'best'}\n",
      "0.389 (+/-0.027) for {'criterion': 'gini', 'max_depth': 15, 'min_samples_leaf': 3, 'min_samples_split': 2, 'splitter': 'random'}\n",
      "0.402 (+/-0.012) for {'criterion': 'gini', 'max_depth': 15, 'min_samples_leaf': 3, 'min_samples_split': 5, 'splitter': 'best'}\n",
      "0.389 (+/-0.027) for {'criterion': 'gini', 'max_depth': 15, 'min_samples_leaf': 3, 'min_samples_split': 5, 'splitter': 'random'}\n",
      "0.392 (+/-0.017) for {'criterion': 'gini', 'max_depth': 15, 'min_samples_leaf': 5, 'min_samples_split': 2, 'splitter': 'best'}\n",
      "0.390 (+/-0.008) for {'criterion': 'gini', 'max_depth': 15, 'min_samples_leaf': 5, 'min_samples_split': 2, 'splitter': 'random'}\n",
      "0.392 (+/-0.017) for {'criterion': 'gini', 'max_depth': 15, 'min_samples_leaf': 5, 'min_samples_split': 5, 'splitter': 'best'}\n",
      "0.390 (+/-0.008) for {'criterion': 'gini', 'max_depth': 15, 'min_samples_leaf': 5, 'min_samples_split': 5, 'splitter': 'random'}\n",
      "0.420 (+/-0.022) for {'criterion': 'entropy', 'max_depth': 20, 'min_samples_leaf': 1, 'min_samples_split': 2, 'splitter': 'best'}\n",
      "0.404 (+/-0.034) for {'criterion': 'entropy', 'max_depth': 20, 'min_samples_leaf': 1, 'min_samples_split': 2, 'splitter': 'random'}\n",
      "0.417 (+/-0.029) for {'criterion': 'entropy', 'max_depth': 20, 'min_samples_leaf': 1, 'min_samples_split': 5, 'splitter': 'best'}\n",
      "0.404 (+/-0.019) for {'criterion': 'entropy', 'max_depth': 20, 'min_samples_leaf': 1, 'min_samples_split': 5, 'splitter': 'random'}\n",
      "0.404 (+/-0.033) for {'criterion': 'entropy', 'max_depth': 20, 'min_samples_leaf': 3, 'min_samples_split': 2, 'splitter': 'best'}\n",
      "0.396 (+/-0.009) for {'criterion': 'entropy', 'max_depth': 20, 'min_samples_leaf': 3, 'min_samples_split': 2, 'splitter': 'random'}\n",
      "0.404 (+/-0.033) for {'criterion': 'entropy', 'max_depth': 20, 'min_samples_leaf': 3, 'min_samples_split': 5, 'splitter': 'best'}\n",
      "0.396 (+/-0.009) for {'criterion': 'entropy', 'max_depth': 20, 'min_samples_leaf': 3, 'min_samples_split': 5, 'splitter': 'random'}\n",
      "0.391 (+/-0.014) for {'criterion': 'entropy', 'max_depth': 20, 'min_samples_leaf': 5, 'min_samples_split': 2, 'splitter': 'best'}\n",
      "0.391 (+/-0.010) for {'criterion': 'entropy', 'max_depth': 20, 'min_samples_leaf': 5, 'min_samples_split': 2, 'splitter': 'random'}\n",
      "0.391 (+/-0.014) for {'criterion': 'entropy', 'max_depth': 20, 'min_samples_leaf': 5, 'min_samples_split': 5, 'splitter': 'best'}\n",
      "0.391 (+/-0.010) for {'criterion': 'entropy', 'max_depth': 20, 'min_samples_leaf': 5, 'min_samples_split': 5, 'splitter': 'random'}\n",
      "0.422 (+/-0.021) for {'criterion': 'entropy', 'max_depth': 18, 'min_samples_leaf': 1, 'min_samples_split': 2, 'splitter': 'best'}\n",
      "0.406 (+/-0.027) for {'criterion': 'entropy', 'max_depth': 18, 'min_samples_leaf': 1, 'min_samples_split': 2, 'splitter': 'random'}\n",
      "0.417 (+/-0.025) for {'criterion': 'entropy', 'max_depth': 18, 'min_samples_leaf': 1, 'min_samples_split': 5, 'splitter': 'best'}\n",
      "0.399 (+/-0.015) for {'criterion': 'entropy', 'max_depth': 18, 'min_samples_leaf': 1, 'min_samples_split': 5, 'splitter': 'random'}\n",
      "0.407 (+/-0.031) for {'criterion': 'entropy', 'max_depth': 18, 'min_samples_leaf': 3, 'min_samples_split': 2, 'splitter': 'best'}\n",
      "0.393 (+/-0.023) for {'criterion': 'entropy', 'max_depth': 18, 'min_samples_leaf': 3, 'min_samples_split': 2, 'splitter': 'random'}\n",
      "0.407 (+/-0.031) for {'criterion': 'entropy', 'max_depth': 18, 'min_samples_leaf': 3, 'min_samples_split': 5, 'splitter': 'best'}\n",
      "0.393 (+/-0.023) for {'criterion': 'entropy', 'max_depth': 18, 'min_samples_leaf': 3, 'min_samples_split': 5, 'splitter': 'random'}\n",
      "0.394 (+/-0.015) for {'criterion': 'entropy', 'max_depth': 18, 'min_samples_leaf': 5, 'min_samples_split': 2, 'splitter': 'best'}\n",
      "0.385 (+/-0.009) for {'criterion': 'entropy', 'max_depth': 18, 'min_samples_leaf': 5, 'min_samples_split': 2, 'splitter': 'random'}\n",
      "0.394 (+/-0.015) for {'criterion': 'entropy', 'max_depth': 18, 'min_samples_leaf': 5, 'min_samples_split': 5, 'splitter': 'best'}\n",
      "0.385 (+/-0.009) for {'criterion': 'entropy', 'max_depth': 18, 'min_samples_leaf': 5, 'min_samples_split': 5, 'splitter': 'random'}\n",
      "0.420 (+/-0.017) for {'criterion': 'entropy', 'max_depth': 15, 'min_samples_leaf': 1, 'min_samples_split': 2, 'splitter': 'best'}\n",
      "0.400 (+/-0.013) for {'criterion': 'entropy', 'max_depth': 15, 'min_samples_leaf': 1, 'min_samples_split': 2, 'splitter': 'random'}\n",
      "0.416 (+/-0.021) for {'criterion': 'entropy', 'max_depth': 15, 'min_samples_leaf': 1, 'min_samples_split': 5, 'splitter': 'best'}\n",
      "0.393 (+/-0.015) for {'criterion': 'entropy', 'max_depth': 15, 'min_samples_leaf': 1, 'min_samples_split': 5, 'splitter': 'random'}\n",
      "0.406 (+/-0.028) for {'criterion': 'entropy', 'max_depth': 15, 'min_samples_leaf': 3, 'min_samples_split': 2, 'splitter': 'best'}\n",
      "0.391 (+/-0.017) for {'criterion': 'entropy', 'max_depth': 15, 'min_samples_leaf': 3, 'min_samples_split': 2, 'splitter': 'random'}\n",
      "0.406 (+/-0.028) for {'criterion': 'entropy', 'max_depth': 15, 'min_samples_leaf': 3, 'min_samples_split': 5, 'splitter': 'best'}\n",
      "0.391 (+/-0.017) for {'criterion': 'entropy', 'max_depth': 15, 'min_samples_leaf': 3, 'min_samples_split': 5, 'splitter': 'random'}\n",
      "0.400 (+/-0.016) for {'criterion': 'entropy', 'max_depth': 15, 'min_samples_leaf': 5, 'min_samples_split': 2, 'splitter': 'best'}\n",
      "0.387 (+/-0.014) for {'criterion': 'entropy', 'max_depth': 15, 'min_samples_leaf': 5, 'min_samples_split': 2, 'splitter': 'random'}\n",
      "0.400 (+/-0.016) for {'criterion': 'entropy', 'max_depth': 15, 'min_samples_leaf': 5, 'min_samples_split': 5, 'splitter': 'best'}\n",
      "0.387 (+/-0.014) for {'criterion': 'entropy', 'max_depth': 15, 'min_samples_leaf': 5, 'min_samples_split': 5, 'splitter': 'random'}\n",
      "Cross-validation f1 macro: 0.422 (+/- 0.021)\n"
     ]
    }
   ],
   "source": [
    "# Выбираем параметры. Тактика у меня одна: сначала берем широкий диапазон параметров, смотрим максимум, а дальше наблюдаем, \n",
    "# какие изменения ведут к улучшению скора, пытаемся их оптимизировать. Иногда направлений движения несколько, смотрим все.\n",
    "# Самая широкая сетка параметров обучается долго, ее я здесь не оставил, тут уже подобранные.\n",
    "# Заметил, что любые параметры max_features вели к ухудшению скора\n",
    "# В данной модели максимум, что я смог получить, это 0.42.\n",
    "param_grid = [{\n",
    "    'min_samples_leaf': [1, 3, 5],\n",
    "    'min_samples_split': [2, 5],\n",
    "    'max_depth': [20, 18, 15],\n",
    "    'criterion': ['gini', 'entropy'],\n",
    "    'splitter': ['best', 'random'],\n",
    "     }]\n",
    "\n",
    "# Поиск по заданной решётке параметров\n",
    "clf = GridSearchCV(tree.DecisionTreeClassifier(random_state=100, class_weight=\"balanced\"),\n",
    "                       param_grid = param_grid, \n",
    "                       scoring='f1_macro', \n",
    "                       verbose=1, \n",
    "                       cv=FOLD_K) \n",
    "\n",
    "# Запускаем поиск\n",
    "clf.fit(X_train, y_train)\n",
    "\n",
    "print(\"Best params on dev set:\")\n",
    "print(clf.best_params_)\n",
    "    \n",
    "print(\"Scores on development set:\")\n",
    "means = clf.cv_results_['mean_test_score']\n",
    "stds = clf.cv_results_['std_test_score']\n",
    "\n",
    "for mean, std, params in zip(means, stds, clf.cv_results_['params']):\n",
    "        print(\"%0.3f (+/-%0.03f) for %r\" % (mean, std * 2, params))\n",
    "    \n",
    "# Обучаем на всём с \"лучшими\" параметрами\n",
    "best_estimator = clf.best_estimator_\n",
    "best_estimator.fit(X_train, y_train)\n",
    "    \n",
    "# Порождаем и сохраняем сабмит\n",
    "y_pred = best_estimator.predict(X_test)\n",
    "submission = pd.DataFrame({\n",
    "    \"ID\": test_data[\"ID\"],\n",
    "    \"Outcome\": y_pred\n",
    "})\n",
    "\n",
    "submission_file_name = f\"submission1.csv\"\n",
    "submission.to_csv(submission_file_name, index=False)\n",
    "\n",
    "# Кросс-валидация\n",
    "cv_scores = cross_val_score(\n",
    "    estimator=best_estimator,\n",
    "    X=X_train,\n",
    "    y=y_train,\n",
    "    cv=FOLD_K,\n",
    "    scoring='f1_macro'\n",
    ")\n",
    "print(f\"Cross-validation f1 macro: {cv_scores.mean():.3f} (+/- {cv_scores.std() * 2:.3f})\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### XGBClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 4 folds for each of 12 candidates, totalling 48 fits\n",
      "Best parameters found on development set:\n",
      "{'gamma': 0.005, 'learning_rate': 0.3, 'max_depth': 11, 'n_estimators': 50, 'reg_alpha': 0.01}\n",
      "\n",
      "Grid scores on development set:\n",
      "0.426 +- 0.016 for {'gamma': 0.005, 'learning_rate': 0.1, 'max_depth': 5, 'n_estimators': 50, 'reg_alpha': 0.01}\n",
      "0.432 +- 0.017 for {'gamma': 0.005, 'learning_rate': 0.1, 'max_depth': 5, 'n_estimators': 100, 'reg_alpha': 0.01}\n",
      "0.442 +- 0.020 for {'gamma': 0.005, 'learning_rate': 0.1, 'max_depth': 11, 'n_estimators': 50, 'reg_alpha': 0.01}\n",
      "0.443 +- 0.011 for {'gamma': 0.005, 'learning_rate': 0.1, 'max_depth': 11, 'n_estimators': 100, 'reg_alpha': 0.01}\n",
      "0.439 +- 0.018 for {'gamma': 0.005, 'learning_rate': 0.1, 'max_depth': 20, 'n_estimators': 50, 'reg_alpha': 0.01}\n",
      "0.439 +- 0.006 for {'gamma': 0.005, 'learning_rate': 0.1, 'max_depth': 20, 'n_estimators': 100, 'reg_alpha': 0.01}\n",
      "0.432 +- 0.018 for {'gamma': 0.005, 'learning_rate': 0.3, 'max_depth': 5, 'n_estimators': 50, 'reg_alpha': 0.01}\n",
      "0.437 +- 0.022 for {'gamma': 0.005, 'learning_rate': 0.3, 'max_depth': 5, 'n_estimators': 100, 'reg_alpha': 0.01}\n",
      "0.444 +- 0.022 for {'gamma': 0.005, 'learning_rate': 0.3, 'max_depth': 11, 'n_estimators': 50, 'reg_alpha': 0.01}\n",
      "0.443 +- 0.016 for {'gamma': 0.005, 'learning_rate': 0.3, 'max_depth': 11, 'n_estimators': 100, 'reg_alpha': 0.01}\n",
      "0.441 +- 0.013 for {'gamma': 0.005, 'learning_rate': 0.3, 'max_depth': 20, 'n_estimators': 50, 'reg_alpha': 0.01}\n",
      "0.436 +- 0.016 for {'gamma': 0.005, 'learning_rate': 0.3, 'max_depth': 20, 'n_estimators': 100, 'reg_alpha': 0.01}\n",
      "Cross-validation f1 macro: 0.455 (+/- 0.020)\n"
     ]
    }
   ],
   "source": [
    "xgb_model = XGBClassifier(\n",
    "    objective='multi:softprob',\n",
    "    num_class=len(np.unique(y_train)),\n",
    "    eval_metric='mlogloss',\n",
    "    random_state=42,\n",
    ")\n",
    "\n",
    "# Тактика выбора параметров такая же. Здесь я усмотрел два направления движения, которые давали похожий результат.\n",
    "# Первый это увеличивать n_estimators, повышать max_depth и добавлять регуляризацию в виде gamma, reg_alpha. Такой вариант тратит больше времени на обучение.\n",
    "# Второй это иметь маленький n_estimators и max_depth пониже c незначительной регуляризацией (Оба варианта я оставил, первый из них закоментил). \n",
    "\n",
    "'''\n",
    "# Первый вариант\n",
    "# Cross-validation f1 macro: 0.448 (+/- 0.024) for {'gamma': 0.04, 'learning_rate': 0.3, 'max_depth': 13, 'n_estimators': 300, 'reg_alpha': 0.05}\n",
    "param_grid = {\n",
    "    'n_estimators': [300],\n",
    "    'learning_rate': [0.3, 0.01],\n",
    "    'max_depth': [5, 13, 20],\n",
    "    'gamma': [0.04, 0.001],\n",
    "    'reg_alpha': [0.05, 0.001],\n",
    "}\n",
    "\n",
    "'''\n",
    "# Второй вариант\n",
    "# Cross-validation f1 macro: 0.455 (+/- 0.020) for {'gamma': 0.005, 'learning_rate': 0.3, 'max_depth': 11, 'n_estimators': 50, 'reg_alpha': 0.01}\n",
    "param_grid = {\n",
    "    'n_estimators': [50, 100],\n",
    "    'learning_rate': [0.1, 0.3],\n",
    "    'max_depth': [5, 11, 20],\n",
    "    'gamma': [0.005],\n",
    "    'reg_alpha': [0.01],\n",
    "}\n",
    "\n",
    "\n",
    "skf = StratifiedKFold(n_splits=FOLD_K, shuffle=True, random_state=42)\n",
    "\n",
    "# Поиск по заданной решётке параметров\n",
    "grid_search = GridSearchCV(\n",
    "    estimator=xgb_model,\n",
    "    param_grid=param_grid,\n",
    "    scoring='f1_macro',\n",
    "    cv=skf,\n",
    "    verbose=1,\n",
    "    n_jobs=-1\n",
    ")\n",
    "\n",
    "# Запускаем поиск\n",
    "grid_search.fit(X_train, y_train)\n",
    "\n",
    "print(\"Best parameters found on development set:\")\n",
    "print(grid_search.best_params_)\n",
    "print()\n",
    "\n",
    "print(\"Grid scores on development set:\")\n",
    "means = grid_search.cv_results_['mean_test_score']\n",
    "stds = grid_search.cv_results_['std_test_score']\n",
    "\n",
    "for mean, std, params in zip(means, stds, grid_search.cv_results_['params']):\n",
    "    print(\"%0.3f +- %0.3f for %r\" % (mean, std * 2, params))\n",
    "\n",
    "# Обучаем на всём с \"лучшими\" параметрами\n",
    "best_model = grid_search.best_estimator_\n",
    "\n",
    "# Порождаем и сохраняем сабмит\n",
    "X_test = test_data_processed.drop('ID', axis=1)\n",
    "y_pred = best_model.predict(X_test)\n",
    "\n",
    "submission = pd.DataFrame({\n",
    "    'ID': test_data['ID'],\n",
    "    'Outcome': y_pred  \n",
    "})\n",
    "\n",
    "submission.to_csv('submission.csv', index=False)\n",
    "\n",
    "# Кросс-валидация\n",
    "cv_scores = cross_val_score(\n",
    "    estimator=best_model,\n",
    "    X=X_train,\n",
    "    y=y_train,\n",
    "    cv=FOLD_K,\n",
    "    scoring='f1_macro'\n",
    ")\n",
    "print(f\"Cross-validation f1 macro: {cv_scores.mean():.3f} (+/- {cv_scores.std() * 2:.3f})\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
